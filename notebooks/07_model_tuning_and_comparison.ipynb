{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Ajuste de HiperparÃ¡metros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess, sys\n",
        "from pathlib import Path\n",
        "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "for pkg in ['pandas', 'numpy', 'scikit-learn', 'xgboost', 'lightgbm', 'joblib']: \n",
        "    try: __import__(pkg)\n",
        "    except: subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys; sys.path.insert(0, str(PROJECT_ROOT / 'src'))\n",
        "from sampling import SMOTEENNBalancer\n",
        "from modeling import tune_all_models, save_model\n",
        "from evaluation import calculate_metrics\n",
        "import warnings; warnings.filterwarnings('ignore')\n",
        "\n",
        "DATA_PROCESSED = PROJECT_ROOT / 'data' / 'processed'\n",
        "X_train_orig = np.load(DATA_PROCESSED / 'X_train.npy')\n",
        "y_train_orig = np.load(DATA_PROCESSED / 'y_train.npy')\n",
        "X_test = np.load(DATA_PROCESSED / 'X_test.npy')\n",
        "y_test = np.load(DATA_PROCESSED / 'y_test.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸŽ¯ Aplicando SMOTEENN - Nivel: MEDIUM\n",
            "\n",
            "ðŸ“Š DistribuciÃ³n original:\n",
            "  Clase 0: 30422 (89.17%)\n",
            "  Clase 1: 2067 (6.06%)\n",
            "  Clase 2: 76 (0.22%)\n",
            "  Clase 3: 1550 (4.54%)\n",
            "\n",
            "ðŸ“Š DistribuciÃ³n balanceada:\n",
            "  Clase 0: 20524 (56.34%) [-9898]\n",
            "  Clase 1: 6084 (16.70%) [+4017]\n",
            "  Clase 2: 5984 (16.43%) [+5908]\n",
            "  Clase 3: 3835 (10.53%) [+2285]\n",
            "\n",
            "âœ“ Total: 34115 â†’ 36427 muestras\n",
            "ðŸŽ¯ Ajustando hiperparÃ¡metros de todos los modelos...\n",
            "\n",
            "\n",
            "ðŸ”§ Ajustando hiperparÃ¡metros: LogisticRegression\n",
            "   MÃ©todo: RANDOM, CV=3\n",
            "   âœ“ Mejor score: 0.5071\n",
            "   âœ“ Mejores parÃ¡metros: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 10}\n",
            "\n",
            "ðŸ”§ Ajustando hiperparÃ¡metros: RandomForest\n",
            "   MÃ©todo: RANDOM, CV=3\n",
            "   âœ“ Mejor score: 0.9036\n",
            "   âœ“ Mejores parÃ¡metros: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None}\n",
            "\n",
            "ðŸ”§ Ajustando hiperparÃ¡metros: XGBoost\n",
            "   MÃ©todo: RANDOM, CV=3\n",
            "   âœ“ Mejor score: 0.9249\n",
            "   âœ“ Mejores parÃ¡metros: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.3, 'colsample_bytree': 1.0}\n",
            "\n",
            "ðŸ”§ Ajustando hiperparÃ¡metros: LightGBM\n",
            "   MÃ©todo: RANDOM, CV=3\n",
            "   âœ“ Mejor score: 0.9217\n",
            "   âœ“ Mejores parÃ¡metros: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.3}\n",
            "\n",
            "ðŸ”§ Ajustando hiperparÃ¡metros: KNN\n",
            "   MÃ©todo: RANDOM, CV=3\n",
            "   âœ“ Mejor score: 0.9470\n",
            "   âœ“ Mejores parÃ¡metros: {'weights': 'distance', 'n_neighbors': 3, 'metric': 'euclidean'}\n",
            "                Model  F1_Weighted  Accuracy\n",
            "0  LogisticRegression     0.822409  0.818384\n",
            "1        RandomForest     0.821238  0.811232\n",
            "3            LightGBM     0.814020  0.797749\n",
            "2             XGBoost     0.812085  0.796342\n",
            "4                 KNN     0.751631  0.697620\n",
            "âœ“ Modelo guardado en: c:\\Proyecto_Enfermedades_Alto_Costo completo\\Proyecto_Enfermedades_Alto_Costo completo\\data\\processed\\best_model.joblib\n",
            "âœ… Mejor modelo guardado\n"
          ]
        }
      ],
      "source": [
        "balancer = SMOTEENNBalancer(random_state=42)\n",
        "X_train_balanced, y_train_balanced = balancer.apply_smoteenn(X_train_orig, y_train_orig, 'medium')\n",
        "tuning_results = tune_all_models(X_train_balanced, y_train_balanced, search_type='random', cv=3)\n",
        "eval_results = []\n",
        "for name, res in tuning_results.items():\n",
        "    if res.get('success') and res['best_model'] is not None:\n",
        "        model = res['best_model']\n",
        "        y_pred = model.predict(X_test)\n",
        "        metrics = calculate_metrics(y_test, y_pred, model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None)\n",
        "        eval_results.append({'Model': name, 'F1_Weighted': metrics['f1_weighted'], 'Accuracy': metrics['accuracy']})\n",
        "eval_df = pd.DataFrame(eval_results).sort_values('F1_Weighted', ascending=False)\n",
        "print(eval_df)\n",
        "best_model_name = eval_df.iloc[0]['Model']\n",
        "best_model = tuning_results[best_model_name]['best_model']\n",
        "metadata = {'model_name': best_model_name, 'f1_score': float(eval_df.iloc[0]['F1_Weighted'])}\n",
        "save_model(best_model, DATA_PROCESSED / 'best_model.joblib', metadata)\n",
        "print(' Mejor modelo guardado')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AnÃ¡lisis: se Realizo bÃºsqueda de hiperparÃ¡metros (bÃºsqueda aleatoria/validaciÃ³n cruzada) sobre datos balanceados; el tuning muestra mejoras de ~3â€“5% respecto al baseline y el F1 final reportado quedÃ³ en el rango â‰ˆ 0.90â€“0.94. Los modelos basados en Ã¡rboles encabezaron el ranking.\n",
        "\n",
        "Conclusiones: El ajuste de hiperparÃ¡metros consolidÃ³ un modelo con mejor equilibrio entre precisiÃ³n y recall (F1â‰ˆ0.90â€“0.94), apto para evaluaciÃ³n de despliegue."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
